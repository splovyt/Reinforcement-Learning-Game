{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    '''Convert an RGB-array to grayscale.'''\n",
    "    r,g,b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    return 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "def reshape_state(img, new_height, new_width):\n",
    "    '''Reshape the state to have the correct input properties for the neural network.'''\n",
    "    # resize image\n",
    "    resized_img = np.array(Image.fromarray(img).resize((new_width, new_height)))\n",
    "    # make grayscale\n",
    "    resized_gray = rgb2gray(resized_img)\n",
    "    # normalize\n",
    "    resized_gray_normalized = resized_gray / 255.0\n",
    "    # reshape for the network\n",
    "    reshaped = resized_gray_normalized.reshape(1,new_height,new_width,1)\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.q_table = deque(maxlen=5000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.buildCNN()\n",
    "        \n",
    "    def buildCNN(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5), input_shape=(64,64,1), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(28, activation='relu'))\n",
    "        # Output layer with two nodes representing Left and Right cart movements\n",
    "        model.add(Dense(self.n_actions, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def write_state(self, state, action, reward, next_state, done):\n",
    "        self.q_table.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.q_table, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times to play the game\n",
    "EPISODES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    # Displays a list of frames as a gif, with controls\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "action_size = env.action_space.n\n",
    "agent = DQN(action_size)\n",
    "batch_size = 32\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    state = env.render(mode='rgb_array')\n",
    "    # feed 64 by 64 grayscale images into CNN\n",
    "    state = reshape_state(state, new_height=64, new_width=64)\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        pix = env.render(mode='rgb_array')\n",
    "        frames.append(pix)\n",
    "        # feed 64 by 64 grayscale images into CNN\n",
    "        next_state = reshape_state(pix, new_height=64, new_width=64)\n",
    "        reward = reward if not done else -10\n",
    "        agent.write_state(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            if time > max_score:\n",
    "                max_score = time\n",
    "                best = frames\n",
    "            break\n",
    "    if len(agent.q_table) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "print(\"Best Score: {}\".format(max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave(\"best.gif\", best, 'GIF', duration=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames_as_gif(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General ##\n",
    "# write out the media files?\n",
    "SAVE_MEDIA = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.999\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Image size: How many pixels for one tile?\n",
    "PIXELS_PER_TILE = 25\n",
    "\n",
    "# How many games to play\n",
    "EPISODES = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "\n",
    "class DQN_RL:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Initialize the RL agent.'''\n",
    "        self.q_table = deque(maxlen=5000) # i.e. the \"training\" data\n",
    "        self.epsilon = EPSILON\n",
    "    \n",
    "    def initialize_actions(self, actions_dict):\n",
    "        # action name to function mapping\n",
    "        self.actions_dict = actions_dict\n",
    "        # action names\n",
    "        self.actions = sorted(list(actions_dict.keys()))\n",
    "        # amount of actions\n",
    "        self.n_actions = len(self.actions)\n",
    "        # give number to each action\n",
    "        self.int2actions = {self.actions.index(x):x for x in self.actions}\n",
    "        self.actions2int = {x:self.actions.index(x) for x in self.actions}\n",
    "    \n",
    "    def buildCNN(self, input_img_height, input_img_width):\n",
    "        kernel_size = (int(input_img_width/game.board.shape[1]), int(input_img_height/game.board.shape[0]))\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(64, kernel_size, input_shape=(input_img_height,input_img_width,1), activation='relu'))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(28, activation='relu'))\n",
    "        # Output layer representing each action\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=LEARNING_RATE))\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        # the exploration vs. exploitation tradeoff\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # exploration\n",
    "            action = random.choice(self.actions)\n",
    "            action_func = self.actions_dict[action]\n",
    "            return action_func\n",
    "        # exploitation\n",
    "        act_values = self.model.predict(state)\n",
    "        action = self.int2actions[np.argmax(act_values[0])]\n",
    "        action_func = self.actions_dict[action]\n",
    "        return action_func  # returns action function\n",
    "\n",
    "    def save_state(self, last_state, action, reward, new_state, done):\n",
    "        '''Add the state, the action, and its effect (reward, next_state, \n",
    "        and whether the game is over) to the Q-table. '''\n",
    "        self.q_table.append((last_state, action, reward, new_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        minibatch = random.sample(self.q_table, BATCH_SIZE)\n",
    "        for last_state, action, reward, new_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + GAMMA *\n",
    "                          np.amax(self.model.predict(new_state)[0]))\n",
    "            target_f = self.model.predict(last_state)\n",
    "            target_f[0][self.actions2int[action]] = target\n",
    "            self.model.fit(last_state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helperfunctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    '''Convert an RGB-array to grayscale.'''\n",
    "    r,g,b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    return 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "def reshape_state(img, new_height, new_width):\n",
    "    '''Reshape the state to have the correct input properties for the neural network.'''\n",
    "    # resize image\n",
    "    resized_img = np.array(Image.fromarray(img).resize((new_width, new_height)))\n",
    "    # make grayscale\n",
    "    resized_gray = rgb2gray(resized_img)\n",
    "    # normalize\n",
    "    resized_gray_normalized = resized_gray / 255.0\n",
    "    # reshape for the network\n",
    "    reshaped = resized_gray_normalized.reshape(1,new_height,new_width,1)\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def reward_function(bombs_placed, terrain_added, win_lose_ongoing):\n",
    "    # if the game ended\n",
    "    if win_lose_ongoing == 'win':\n",
    "        return 100\n",
    "    elif win_lose_ongoing == 'lose': # draw is a loss too\n",
    "        return -100\n",
    "    \n",
    "    # if nothing happened, place a penalty for duration of the game\n",
    "    elif bombs_placed == 0 and terrain_added == 0:\n",
    "        return -1\n",
    "    \n",
    "    # calculate the reward\n",
    "    else:\n",
    "        return bombs_placed * 10 + terrain_added * 10\n",
    "\n",
    "def calculate_bombs_placed(json):\n",
    "    return len([x for x in json['board_positions']['bombs_and_stage'] if x[2] == 1])\n",
    "\n",
    "def calculate_terrain_added(json, last_json):\n",
    "    # extract the frame\n",
    "    frame = json['game_properties']['frame']\n",
    "    \n",
    "    # terrain added\n",
    "    if frame == 1:\n",
    "        previous_amount_of_terrain = len(json['board_positions']['land'])\n",
    "    else:\n",
    "        previous_amount_of_terrain = len(last_json['board_positions']['land'])\n",
    "\n",
    "    terrain_added = len(json['board_positions']['land']) - previous_amount_of_terrain\n",
    "    \n",
    "    return terrain_added\n",
    "    \n",
    "def calculate_reward(json, last_json, win_lose_ongoing):\n",
    "    # bombs placed this frame\n",
    "    bombs_placed = calculate_bombs_placed(json)\n",
    "\n",
    "    # terrain added this frame\n",
    "    terrain_added = calculate_terrain_added(json, last_json)\n",
    "    \n",
    "    # reward in this state\n",
    "    return reward_function(bombs_placed, terrain_added, win_lose_ongoing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play/Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source imports\n",
    "from game import Game, Player\n",
    "from render_tool import RenderTool, MapScheme\n",
    "\n",
    "# additional library imports\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dev imports\n",
    "from utils import save_dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "RL_performance = []\n",
    "\n",
    "# initialize the agent\n",
    "agent_player1 = DQN_RL()\n",
    "agent_player2 = DQN_RL()\n",
    "\n",
    "# start\n",
    "for e in tqdm(range(EPISODES)):\n",
    "    # choose the map\n",
    "    #map = MapScheme().IBM\n",
    "    map = MapScheme().standard\n",
    "\n",
    "    # initialize the game\n",
    "    game = Game(map, verbose=False)\n",
    "    RT = RenderTool(game)\n",
    "    \n",
    "    # name the players\n",
    "    player1 = Player(game, 'Sonic')\n",
    "    player2 = Player(game, 'Knuckles')\n",
    "    \n",
    "    # initialize the action space\n",
    "    agent_player1.initialize_actions(actions_dict={'Up': player1.Up,\n",
    "                                                 'Down': player1.Down,\n",
    "                                                 'Left': player1.Left,\n",
    "                                                 'Right': player1.Right,\n",
    "                                                 'Still': player1.Still,\n",
    "                                                 'Bomb': player1.Bomb})\n",
    "    agent_player2.initialize_actions(actions_dict={'Up': player2.Up,\n",
    "                                                 'Down': player2.Down,\n",
    "                                                 'Left': player2.Left,\n",
    "                                                 'Right': player2.Right,\n",
    "                                                 'Still': player2.Still,\n",
    "                                                 'Bomb': player2.Bomb})\n",
    "    \n",
    "    # calculate the properties of the input images\n",
    "    pixel_height_tile = math.ceil(math.sqrt(PIXELS_PER_TILE))\n",
    "    pixel_width_tile = pixel_height_tile\n",
    "    input_image_height = pixel_height_tile * game.board.shape[0]\n",
    "    input_image_width = pixel_width_tile * game.board.shape[1]\n",
    "    \n",
    "    # build the CNN\n",
    "    agent_player1.buildCNN(input_image_height, input_image_width)\n",
    "    agent_player2.buildCNN(input_image_height, input_image_width)\n",
    "\n",
    "    # start the game (frame 1)\n",
    "    if game.start():\n",
    "        # update the frame\n",
    "        game_status_dict = game.get_status_dict()\n",
    "        save_dict('data/{}/{}.pickle'.format(game.id, game.frame), game_status_dict)\n",
    "        \n",
    "        # render the frame\n",
    "        frame = np.array(RT.render_current_frame(SAVE_MEDIA))\n",
    "        \n",
    "        # reshape the frame (= the state)\n",
    "        new_state = reshape_state(frame, new_height=input_image_height, new_width=input_image_width)\n",
    "        \n",
    "        # cumulative reward\n",
    "        cumulative_reward_player1 = 0\n",
    "        cumulative_reward_player2 = 0\n",
    "        \n",
    "        # track performance\n",
    "        total_terrain_added = 0\n",
    "        total_bombs_placed = 0\n",
    "        \n",
    "    while game_status_dict['game_properties']['outcome'] == 'ongoing':\n",
    "        # set the latest new state to the last state\n",
    "        last_state = new_state\n",
    "        last_game_status_dict = game_status_dict\n",
    "        \n",
    "        # select an action for the player\n",
    "        move_player1 = agent_player1.act(last_state)\n",
    "        move_player2 = agent_player2.act(last_state)\n",
    "        \n",
    "        move_player1()\n",
    "        move_player2()\n",
    "\n",
    "        # update the frame\n",
    "        game.update_frame()\n",
    "        game_status_dict = game.get_status_dict()\n",
    "        save_dict('data/{}/{}.pickle'.format(game.id, game.frame), game_status_dict)\n",
    "\n",
    "        # render the frame\n",
    "        frame = np.array(RT.render_current_frame(SAVE_MEDIA))\n",
    "        \n",
    "        # reshape the frame (= the state)\n",
    "        new_state = reshape_state(frame, new_height=input_image_height, new_width=input_image_width)\n",
    "        \n",
    "        # update the Q-table\n",
    "        done = False\n",
    "        if game.ended:\n",
    "            done = True\n",
    "            \n",
    "        if game_status_dict['game_properties']['outcome'] == 'ongoing':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='ongoing')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='ongoing')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'draw':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'player_1':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='win')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'player_2':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='win')\n",
    "            \n",
    "        agent_player1.save_state(last_state=last_state, \n",
    "                                 action=move_player1.__name__,\n",
    "                                 reward=reward_player1, \n",
    "                                 new_state=new_state,\n",
    "                                 done=done)\n",
    "        agent_player2.save_state(last_state=last_state, \n",
    "                                 action=move_player2.__name__,\n",
    "                                 reward=reward_player2, \n",
    "                                 new_state=new_state,\n",
    "                                 done=done)\n",
    "        \n",
    "        cumulative_reward_player1 += reward_player1\n",
    "        cumulative_reward_player2 += reward_player2\n",
    "        \n",
    "        # track performance\n",
    "        total_terrain_added += calculate_terrain_added(json=game_status_dict, last_json=last_game_status_dict)\n",
    "        total_bombs_placed += calculate_bombs_placed(json=game_status_dict)\n",
    "\n",
    "    if len(agent_player1.q_table) > BATCH_SIZE:\n",
    "        agent_player1.replay()\n",
    "        agent_player2.replay()\n",
    "\n",
    "    # track performance\n",
    "    print(cumulative_reward_player1, cumulative_reward_player2, agent_player1.epsilon)\n",
    "    \n",
    "    RL_performance.append([agent_player1.epsilon, cumulative_reward_player1, cumulative_reward_player2, \n",
    "                           game_status_dict['game_properties']['outcome'], len(game.players[0].history),\n",
    "                          total_terrain_added, total_bombs_placed])\n",
    "    \n",
    "    if e > 0 and e % 10 == 0:\n",
    "        pd.DataFrame(RL_performance, columns=['epsilon', 'cum_reward_p1', 'cum_reward_p2',\n",
    "                                             'outcome', 'game_length', 'total_terrain_added',\n",
    "                                             'total_bombs_placed']).to_csv('RL_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement-Learning-Game-DEkug-dS",
   "language": "python",
   "name": "reinforcement-learning-game-dekug-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
