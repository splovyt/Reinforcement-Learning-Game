{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    '''Convert an RGB-array to grayscale.'''\n",
    "    r,g,b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    return 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "def reshape_state(img, new_height, new_width):\n",
    "    '''Reshape the state to have the correct input properties for the neural network.'''\n",
    "    # resize image\n",
    "    resized_img = np.array(Image.fromarray(img).resize((new_width, new_height)))\n",
    "    # make grayscale\n",
    "    resized_gray = rgb2gray(resized_img)\n",
    "    # normalize\n",
    "    resized_gray_normalized = resized_gray / 255.0\n",
    "    # reshape for the network\n",
    "    reshaped = resized_gray_normalized.reshape(1,new_height,new_width,1)\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.q_table = deque(maxlen=5000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.buildCNN()\n",
    "        \n",
    "    def buildCNN(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5), input_shape=(64,64,1), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(28, activation='relu'))\n",
    "        # Output layer with two nodes representing Left and Right cart movements\n",
    "        model.add(Dense(self.n_actions, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def write_state(self, state, action, reward, next_state, done):\n",
    "        self.q_table.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.q_table, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times to play the game\n",
    "EPISODES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    # Displays a list of frames as a gif, with controls\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "action_size = env.action_space.n\n",
    "agent = DQN(action_size)\n",
    "batch_size = 32\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    state = env.render(mode='rgb_array')\n",
    "    # feed 64 by 64 grayscale images into CNN\n",
    "    state = reshape_state(state, new_height=64, new_width=64)\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        pix = env.render(mode='rgb_array')\n",
    "        frames.append(pix)\n",
    "        # feed 64 by 64 grayscale images into CNN\n",
    "        next_state = reshape_state(pix, new_height=64, new_width=64)\n",
    "        reward = reward if not done else -10\n",
    "        agent.write_state(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            if time > max_score:\n",
    "                max_score = time\n",
    "                best = frames\n",
    "            break\n",
    "    if len(agent.q_table) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "print(\"Best Score: {}\".format(max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave(\"best.gif\", best, 'GIF', duration=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames_as_gif(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General ##\n",
    "# write out the media files?\n",
    "SAVE_MEDIA = False\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Image size: How many pixels for one tile?\n",
    "PIXELS_PER_TILE = 25\n",
    "\n",
    "# How many games to play\n",
    "EPISODES = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "\n",
    "class DQN_RL:\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Initialize the RL agent.'''\n",
    "        self.q_table = deque(maxlen=5000) # i.e. the \"training\" data\n",
    "        self.epsilon = EPSILON\n",
    "    \n",
    "    def initialize_actions(self, actions_dict):\n",
    "        # action name to function mapping\n",
    "        self.actions_dict = actions_dict\n",
    "        # action names\n",
    "        self.actions = sorted(list(actions_dict.keys()))\n",
    "        # amount of actions\n",
    "        self.n_actions = len(self.actions)\n",
    "        # give number to each action\n",
    "        self.int2actions = {self.actions.index(x):x for x in self.actions}\n",
    "        self.actions2int = {x:self.actions.index(x) for x in self.actions}\n",
    "    \n",
    "    def buildCNN(self, input_img_height, input_img_width):\n",
    "        kernel_size = (int(input_img_width/game.board.shape[1]), int(input_img_height/game.board.shape[0]))\n",
    "        pixels_in_image = game.board.shape[0] * game.board.shape[1]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        # try to mimic one tile as one kernel\n",
    "        self.model.add(Conv2D(pixels_in_image, kernel_size, input_shape=(input_img_height,input_img_width,1), \n",
    "                              strides=int(input_img_width/game.board.shape[1]), activation='relu'))\n",
    "        # try to mimic a combination of tiles as a kernel\n",
    "        self.model.add(Conv2D(int(pixels_in_image // 4), kernel_size=(5,5), input_shape=(input_img_height,input_img_width,1), \n",
    "                              activation='relu'))\n",
    "        # flatten the layers\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(28, activation='relu'))\n",
    "        # Output layer representing each action\n",
    "        self.model.add(Dense(self.n_actions, activation='linear'))\n",
    "        self.model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=LEARNING_RATE))\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        # the exploration vs. exploitation tradeoff\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # exploration\n",
    "            action = random.choice(self.actions)\n",
    "            action_func = self.actions_dict[action]\n",
    "            return action_func\n",
    "        # exploitation\n",
    "        act_values = self.model.predict(state)\n",
    "        action = self.int2actions[np.argmax(act_values[0])]\n",
    "        action_func = self.actions_dict[action]\n",
    "        return action_func  # returns action function\n",
    "\n",
    "    def save_state(self, last_state, action, reward, new_state, done):\n",
    "        '''Add the state, the action, and its effect (reward, next_state, \n",
    "        and whether the game is over) to the Q-table. '''\n",
    "        self.q_table.append((last_state, action, reward, new_state, done))\n",
    "        \n",
    "    def save_model(self, fname):\n",
    "        self.model.save(fname)\n",
    "    \n",
    "    def replay(self):\n",
    "        minibatch = random.sample(self.q_table, BATCH_SIZE)\n",
    "        for last_state, action, reward, new_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + GAMMA *\n",
    "                          np.amax(self.model.predict(new_state)[0]))\n",
    "            target_f = self.model.predict(last_state)\n",
    "            target_f[0][self.actions2int[action]] = target\n",
    "            self.model.fit(last_state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helperfunctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    '''Convert an RGB-array to grayscale.'''\n",
    "    r,g,b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    return 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "def reshape_state(img, new_height, new_width):\n",
    "    '''Reshape the state to have the correct input properties for the neural network.'''\n",
    "    # resize image\n",
    "    resized_img = np.array(Image.fromarray(img).resize((new_width, new_height)))\n",
    "    # make grayscale\n",
    "    resized_gray = rgb2gray(resized_img)\n",
    "    # normalize\n",
    "    resized_gray_normalized = resized_gray / 255.0\n",
    "    # reshape for the network\n",
    "    reshaped = resized_gray_normalized.reshape(1,new_height,new_width,1)\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def reward_function(bombs_placed, terrain_added, win_lose_ongoing):\n",
    "    # if the game ended\n",
    "    if win_lose_ongoing == 'win':\n",
    "        return 100\n",
    "    elif win_lose_ongoing == 'lose': # draw is a loss too\n",
    "        return -100\n",
    "    \n",
    "    # if nothing happened, place a penalty for duration of the game\n",
    "    elif bombs_placed == 0 and terrain_added == 0:\n",
    "        return -1\n",
    "    \n",
    "    # calculate the reward\n",
    "    else:\n",
    "        return bombs_placed * 10 + terrain_added * 10\n",
    "\n",
    "def calculate_bombs_placed(json):\n",
    "    return len([x for x in json['board_positions']['bombs_and_stage'] if x[2] == 1])\n",
    "\n",
    "def calculate_terrain_added(json, last_json):\n",
    "    # extract the frame\n",
    "    frame = json['game_properties']['frame']\n",
    "    \n",
    "    # terrain added\n",
    "    if frame == 1:\n",
    "        previous_amount_of_terrain = len(json['board_positions']['land'])\n",
    "    else:\n",
    "        previous_amount_of_terrain = len(last_json['board_positions']['land'])\n",
    "\n",
    "    terrain_added = len(json['board_positions']['land']) - previous_amount_of_terrain\n",
    "    \n",
    "    return terrain_added\n",
    "    \n",
    "def calculate_reward(json, last_json, win_lose_ongoing):\n",
    "    # bombs placed this frame\n",
    "    bombs_placed = calculate_bombs_placed(json)\n",
    "\n",
    "    # terrain added this frame\n",
    "    terrain_added = calculate_terrain_added(json, last_json)\n",
    "    \n",
    "    # reward in this state\n",
    "    return reward_function(bombs_placed, terrain_added, win_lose_ongoing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play/Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 341/5000 [3:25:27<100:27:09, 77.62s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8f043e8db105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mmove_player1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#move_player1 = agent_player1.act(last_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mmove_player2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_player2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mmove_player1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7b749847ba92>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maction_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# exploitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint2actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0maction_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Reinforcement-Learning-Game-DEkug-dS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# source imports\n",
    "from game import Game, Player\n",
    "from render_tool import RenderTool, MapScheme\n",
    "\n",
    "# additional library imports\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dev-RL-solution imports\n",
    "from utils import save_dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "RL_performance = []\n",
    "\n",
    "# initialize the agent\n",
    "#agent_player1 = DQN_RL()\n",
    "agent_player2 = DQN_RL()\n",
    "\n",
    "# start\n",
    "for e in tqdm(range(EPISODES)):\n",
    "    # choose the map\n",
    "    # map = MapScheme().IBM\n",
    "    map = MapScheme().standard\n",
    "\n",
    "    # initialize the game\n",
    "    game = Game(map, verbose=False)\n",
    "    RT = RenderTool(game)\n",
    "\n",
    "    # name the players\n",
    "    player1 = Player(game, 'Sonic')\n",
    "    player2 = Player(game, 'Knuckles')\n",
    "\n",
    "    # initialize the action space\n",
    "    #agent_player1.initialize_actions(actions_dict={'Up': player1.Up,\n",
    "    #                                               'Down': player1.Down,\n",
    "    #                                               'Left': player1.Left,\n",
    "    #                                               'Right': player1.Right,\n",
    "    #                                               #'Still': player1.Still,\n",
    "    #                                               'Bomb': player1.Bomb})\n",
    "    agent_player2.initialize_actions(actions_dict={'Up': player2.Up,\n",
    "                                                   'Down': player2.Down,\n",
    "                                                   'Left': player2.Left,\n",
    "                                                   'Right': player2.Right,\n",
    "                                                   #'Still': player2.Still,\n",
    "                                                   'Bomb': player2.Bomb})\n",
    "\n",
    "    # calculate the properties of the input images\n",
    "    pixel_height_tile = math.ceil(math.sqrt(PIXELS_PER_TILE))\n",
    "    pixel_width_tile = pixel_height_tile\n",
    "    input_image_height = pixel_height_tile * game.board.shape[0]\n",
    "    input_image_width = pixel_width_tile * game.board.shape[1]\n",
    "\n",
    "    # build the CNN\n",
    "    #agent_player1.buildCNN(input_image_height, input_image_width)\n",
    "    agent_player2.buildCNN(input_image_height, input_image_width)\n",
    "\n",
    "    # start the game (frame 1)\n",
    "    if game.start():\n",
    "        # update the frame\n",
    "        game_status_dict = game.get_status_dict()\n",
    "        save_dict('data/{}/{}.pickle'.format(game.id, game.frame), game_status_dict)\n",
    "\n",
    "        # render the frame\n",
    "        frame = np.array(RT.render_current_frame(SAVE_MEDIA))\n",
    "\n",
    "        # reshape the frame (= the state)\n",
    "        new_state = reshape_state(frame, new_height=input_image_height, new_width=input_image_width)\n",
    "\n",
    "        # cumulative reward\n",
    "        #cumulative_reward_player1 = 0\n",
    "        cumulative_reward_player2 = 0\n",
    "\n",
    "        # track performance\n",
    "        total_terrain_added = 0\n",
    "        total_bombs_placed = 0\n",
    "\n",
    "    while game_status_dict['game_properties']['outcome'] == 'ongoing':\n",
    "        # set the latest new state to the last state\n",
    "        last_state = new_state\n",
    "        last_game_status_dict = game_status_dict\n",
    "\n",
    "        # select an action for the player\n",
    "        move_player1 = random.choice([player1.Up, player1.Down, player1.Right, player1.Left])\n",
    "        #move_player1 = agent_player1.act(last_state)\n",
    "        move_player2 = agent_player2.act(last_state)\n",
    "\n",
    "        move_player1()\n",
    "        move_player2()\n",
    "\n",
    "        # update the frame\n",
    "        game.update_frame()\n",
    "        game_status_dict = game.get_status_dict()\n",
    "        save_dict('data/{}/{}.pickle'.format(game.id, game.frame), game_status_dict)\n",
    "\n",
    "        # render the frame\n",
    "        frame = np.array(RT.render_current_frame(SAVE_MEDIA))\n",
    "\n",
    "        # reshape the frame (= the state)\n",
    "        new_state = reshape_state(frame, new_height=input_image_height, new_width=input_image_width)\n",
    "\n",
    "        # update the Q-table\n",
    "        done = False\n",
    "        if game.ended:\n",
    "            done = True\n",
    "\n",
    "        if game_status_dict['game_properties']['outcome'] == 'ongoing':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='ongoing')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='ongoing')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'draw':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'player_1':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='win')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "        elif game_status_dict['game_properties']['outcome'] == 'player_2':\n",
    "            reward_player1 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='lose')\n",
    "            reward_player2 = calculate_reward(json=game_status_dict, last_json=last_game_status_dict,\n",
    "                                              win_lose_ongoing='win')\n",
    "\n",
    "        #agent_player1.save_state(last_state=last_state,\n",
    "        #                         action=move_player1.__name__,\n",
    "        #                         reward=reward_player1,\n",
    "        #                         new_state=new_state,\n",
    "        #                         done=done)\n",
    "        agent_player2.save_state(last_state=last_state,\n",
    "                                 action=move_player2.__name__,\n",
    "                                 reward=reward_player2,\n",
    "                                 new_state=new_state,\n",
    "                                 done=done)\n",
    "\n",
    "        #cumulative_reward_player1 += reward_player1\n",
    "        cumulative_reward_player2 += reward_player2\n",
    "\n",
    "        # track performance\n",
    "        total_terrain_added += calculate_terrain_added(json=game_status_dict, last_json=last_game_status_dict)\n",
    "        total_bombs_placed += calculate_bombs_placed(json=game_status_dict)\n",
    "\n",
    "    # require at least some terrain addition\n",
    "    if len(agent_player2.q_table) > BATCH_SIZE:\n",
    "        #agent_player1.replay()\n",
    "        agent_player2.replay()\n",
    "        agent_player2.save_model('player2model.h5')\n",
    "\n",
    "    # track performance\n",
    "    RL_performance.append([game.id, round(agent_player2.epsilon,2), 0, cumulative_reward_player2,\n",
    "                           game_status_dict['game_properties']['outcome'], len(game.players[1].history),\n",
    "                           total_terrain_added, total_bombs_placed])\n",
    "    #RL_performance.append([game.id, round(agent_player2.epsilon,2), cumulative_reward_player1, cumulative_reward_player2,\n",
    "    #                       game_status_dict['game_properties']['outcome'], len(game.players[1].history),\n",
    "    #                       total_terrain_added, total_bombs_placed])\n",
    "\n",
    "    if e > 0 and e % 10 == 0:\n",
    "        pd.DataFrame(RL_performance, columns=['game_id','epsilon', 'cum_reward_p1', 'cum_reward_p2',\n",
    "                                              'outcome', 'game_length', 'total_terrain_added',\n",
    "                                              'total_bombs_placed']).to_csv('RL_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement-Learning-Game-DEkug-dS",
   "language": "python",
   "name": "reinforcement-learning-game-dekug-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
